{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/concys/ner-harem/test_2018-9-5_fix.iob') as f:\n",
    "#with open('output/test_dataset_docid.pos') as f:\n",
    "    documents = []\n",
    "    paragraphs = []\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    poses = []\n",
    "    \n",
    "    for line in f:\n",
    "        line = line[:-1]\n",
    "        #if \"PPM', 'PNOUN'\" in line:\n",
    "        #    set_trace()\n",
    "        \n",
    "        if line != 'S' and line != 'P' and not line.startswith('D'):\n",
    "            tokens.append(line.split(', ')[0][1:-1])\n",
    "            poses.append(line.split(', ')[1][1:-1])\n",
    "        elif line == 'S':\n",
    "            sentences.append(list(zip(tokens, poses)))\n",
    "            tokens = []\n",
    "            poses = []\n",
    "        elif line == 'P':\n",
    "            paragraphs.append(sentences)\n",
    "            sentences = []\n",
    "        elif line.startswith('D'):\n",
    "            docid = line.split()[1]\n",
    "            documents.append((paragraphs, docid))\n",
    "            paragraphs = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents = documents[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharInformation(sentences):\n",
    "    new_sentences = []\n",
    "    for s in sentences:\n",
    "        new_sentence = []\n",
    "        for data in s:\n",
    "            token = data[0]\n",
    "            chars = [c for c in token]\n",
    "            new_sentence.append([data[0], data[1], chars])\n",
    "        new_sentences.append(new_sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "\n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "\n",
    "def createMatrices(sentences, word2Idx, case2Idx,char2Idx, pos2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        posIndices = []\n",
    "        \n",
    "        for word, pos, char in sentence: \n",
    "            wordCount += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                # makes an exception for non-breaking space\n",
    "                # add this character to char2Idx later\n",
    "                x = x.replace('\\xa0', ' ')\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            posIndices.append(pos2Idx[pos])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, posIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "word2Idx = pickle.load(open('word2Idx.pkl', 'rb'))\n",
    "case2Idx = pickle.load(open('case2Idx.pkl', 'rb'))\n",
    "char2Idx = pickle.load(open('char2Idx.pkl', 'rb'))\n",
    "pos2Idx = pickle.load(open('pos2Idx.pkl', 'rb'))\n",
    "idx2Label = pickle.load(open('idx2L.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrices = padding(createMatrices(char_sentences, word2Idx, case2Idx, char2Idx, pos2Idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model('/home/concys/ner-harem/model_with_pos_v2.bin')\n",
    "model = keras.models.load_model('/home/concys/ner-harem/model_without_pos.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_labels(matrices):\n",
    "    labels = []\n",
    "    for data in matrices:\n",
    "        #print(data)\n",
    "        tokens, casing, char, pos = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pos = np.asarray([pos])\n",
    "\n",
    "        #print('tokens', tokens)\n",
    "        #print('casing', casing)\n",
    "        #print('char', char)\n",
    "        #print('pos', pos)\n",
    "\n",
    "\n",
    "\n",
    "        #pred = model.predict([tokens, casing, pos, char], verbose=False)[0]   \n",
    "        pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "        pred = pred.argmax(axis=-1) #Predict the classes\n",
    "        #print('Labels:')\n",
    "        #print(pred)\n",
    "        labels.append([idx2Label[p] for p in pred])\n",
    "        \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text(labels, id_counter, text, out_tree):\n",
    "    has_found_em = False\n",
    "    is_outside_em = True\n",
    "    for label, token in zip(labels, text):\n",
    "        #label = label[0]\n",
    "        if label.startswith('B_'):\n",
    "            category = label.split('_')[1]\n",
    "            out_em = ET.SubElement(out_tree, 'EM', attrib={'ID': str(id_counter), 'CATEG': category})\n",
    "            out_em.tail = ''\n",
    "            id_counter += 1\n",
    "            out_em.text = token\n",
    "            has_found_em = True\n",
    "            is_outside_em = False\n",
    "        elif label.startswith('I_'):\n",
    "            if not is_outside_em:\n",
    "                out_em.text += ' ' + token\n",
    "            else:\n",
    "                category = label.split('_')[1]\n",
    "                out_em = ET.SubElement(out_tree, 'EM', attrib={'ID': str(id_counter), 'CATEG': category})\n",
    "                out_em.tail = ''\n",
    "                id_counter += 1\n",
    "                out_em.text = ' ' + token\n",
    "            has_found_em = True\n",
    "        else:\n",
    "            if not is_outside_em:\n",
    "                is_outside_em = True\n",
    "                out_em.tail += ' ' + token\n",
    "            else:\n",
    "                if not has_found_em:\n",
    "                    if out_tree.text:\n",
    "                        out_tree.text += ' ' + token\n",
    "                    else:\n",
    "                        out_tree.text = token\n",
    "                else:\n",
    "                    out_em.tail += ' ' + token\n",
    "                \n",
    "    return id_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s, l in zip(sentences, labels):\n",
    "#    for token, l_token in zip(s, l):\n",
    "#        print(token[0], l_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "id_counter = 0\n",
    "out_tree = ET.Element('colHAREM')\n",
    "out_tree.set('versao', 'ColeccaoSegundoHAREM-4.0')\n",
    "for document in documents:\n",
    "    out_doc = ET.SubElement(out_tree, 'DOC')\n",
    "    # add DOCID later\n",
    "    out_doc.set('DOCID', document[1])\n",
    "    for p in document[0]:\n",
    "        out_p = ET.SubElement(out_doc, 'P')\n",
    "        \n",
    "        sentences = p\n",
    "        #if any(('PPM', 'PNOUN') in s for s in sentences):\n",
    "            \n",
    "        #    set_trace()\n",
    "        matrices = padding(createMatrices(addCharInformation(sentences), word2Idx, case2Idx, char2Idx, pos2Idx))\n",
    "\n",
    "        labels = predict_labels(matrices)\n",
    "        labels = [i for l in labels for i in l]\n",
    "\n",
    "        \n",
    "        #print(labels)\n",
    "        \n",
    "        text = [i[0] for l in sentences for i in l]\n",
    "        #print(text)\n",
    "        id_counter = annotate_text(labels, id_counter, text, out_p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = ET.ElementTree(out_tree)\n",
    "\n",
    "ot.write('harem_without_pos_fix.xml', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = padding(createMatrices(addCharInformation([['O', 'Pedro', 'mora', 'em', 'Porto', 'Alegre', 'e', 'estuda', 'na', 'UFRGS', '.']]), word2Idx, case2Idx, char2Idx, pos2Idx))\n",
    "labels = predict_labels(matrices)\n",
    "\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
