{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_utils as MLU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Model, Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding\n",
    "from keras.layers import Dropout, LSTM, Bidirectional, MaxPooling1D\n",
    "from keras.layers import Input, concatenate, Concatenate, Flatten\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ignore Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load PT Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pt_w2v_patch = 'C:\\word_embedding\\pt_cbow_s300.txt'\n",
    "pt_w2v = KeyedVectors.load_word2vec_format(pt_w2v_patch, unicode_errors=\"ignore\")\n",
    "pt_model_w2v = {w: vec for w, vec in zip(pt_w2v.index2word, pt_w2v.syn0)}\n",
    "del pt_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load EN Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "en_w2v_patch = 'C:\\word_embedding\\GoogleNews-vectors-negative300.bin'\n",
    "en_w2v = gensim.models.Word2Vec.load_word2vec_format(en_w2v_patch, binary=True)\n",
    "en_model_w2v = {w: vec for w, vec in zip(en_w2v.index2word, en_w2v.syn0)}\n",
    "del en_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Harem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.open_csv('harem.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.open_csv('conll2003.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PT Word2vec Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w2v'] = [pt_w2v[word] for word in df['word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EN Word2Vec Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w2v'] = [en_w2v[word] for word in df['word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POS Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_, pos2idx = MLU.myHotEncode([df['pos']])\n",
    "df['Pos'] = data_[0]\n",
    "del data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graphic Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_, graphic2idx = MLU.myHotEncode([df['Graphic']])\n",
    "df['Graphic'] = data_[0]\n",
    "del data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIO Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_classes = []\n",
    "for bio, classe in zip(df['bio'], df['class']):\n",
    "    if classe is None or classe.strip() == '':\n",
    "        bio_classes.append('O')\n",
    "    else:\n",
    "        bio_classes.append(classe + '-' + bio)\n",
    "df['bio_class'] = bio_classes\n",
    "del bio_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIOSE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biose_classes = []\n",
    "for biose, classe in zip(df['biose'], df['class']):\n",
    "    if classe is None or classe.strip() == '':\n",
    "        biose_classes.append('O')\n",
    "    else:\n",
    "        biose_classes.append(classe + '-' + biose)\n",
    "df['biose_class'] = biose_classes\n",
    "del biose_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemple_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_dim=1000, name='LSTM0'))\n",
    "    model.add(Dense(256, activation='relu', name='Dence1'))\n",
    "    model.add(Dropout(0.5, name='Droupout2'))\n",
    "    model.add(Dense(7, activation='sigmoid', name='Dense_out3'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "# ToDo (Felipe) - Diferentes funções que criam diferentes modelos que utilizam \n",
    "# features distintas, como CNN (morfológica), LSTM (Word2vec), LSTM (Graphics) ...\n",
    "# Criar os modelos baseados nos modelos do trabalho CNN+LSTN+Bidirecional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(run_id, model, df, features, class_column, k=5, random_state=0, \n",
    "                  metric_average=\"macro\", epochs=10, verbose=1):\n",
    "    \n",
    "    start_benchmark = time.time()\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    confusion = []\n",
    "    execution_time = []\n",
    "    test_time = []\n",
    "    \n",
    "    gp = GroupKFold(n_splits=k)\n",
    "    for train_indexs, test_indexs in gp.split(df['Word'], groups=df['sentence_code']):\n",
    "        x_train = []\n",
    "        for feature in features:\n",
    "            x_train.append(df[feature][train_indexs])\n",
    "        y_train = df[class_column][train_indexs]\n",
    "        \n",
    "        x_test = []\n",
    "        for feature in features:\n",
    "            x_test.append(df[feature][test_indexs])\n",
    "        y_test = df[class_column][test_indexs]\n",
    "                \n",
    "        start_time = time.time()\n",
    "        model_ = model.fit(x_train, y_train, verbose=verbose, epochs=epochs)\n",
    "        end_time = time.time() - start_time\n",
    "        execution_time.append(end_time)\n",
    "                \n",
    "        start_time = time.time()\n",
    "        result = model_.predict(x_test)\n",
    "        end_time = time.time() - start_time\n",
    "        test_time.append(end_time)\n",
    "\n",
    "        accuracy.append(accuracy_score(result, y_test))\n",
    "        precision.append(precision_score(result, y_test, average=metric_average))\n",
    "        recall.append(recall_score(result, y_test, average=metric_average))\n",
    "        f1.append(f1_score(result, y_test, average=metric_average))\n",
    "        confusion.append(confusion_matrix(result, y_test))\n",
    "    \n",
    "    print('')\n",
    "    aux = time.time() - start_benchmark\n",
    "    print('Run time benchmark:', aux)\n",
    "    \n",
    "    results = {\n",
    "        'run_id': run_id,\n",
    "        'datetime': datetime.datetime.now(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion': confusion,\n",
    "        'train_time': execution_time,\n",
    "        'test_time': test_time,\n",
    "        'benchmark_time': aux\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "df_result = run_benchmark('example_run', exemple_lstm_model(), df, ['w2v'], 'bio_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUIDADO PRA NÃO SALVAR EM CIMA DE UM ARQUIVO COM RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sempre muda o nome do arquivo pra não salvar em cima !\n",
    "df_result.to_csv('results/exemplo_070219.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
